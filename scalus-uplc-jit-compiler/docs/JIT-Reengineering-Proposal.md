# JIT Compiler Re-engineering Proposal

## Problem Statement

The current JIT interpreter in `scalus-uplc-jit-compiler` has **the same performance as CekMachine** in `scalus-core`, which defeats the purpose of JIT compilation. While the minimal continuation approach was an improvement over the previous CallCCTrampoline design, we're not seeing the expected performance gains.

## Current State Analysis

### Architecture Overview

**Current JIT Approach:**
1. **Code Generation Phase** (compile-time via Scala 3 staging)
   - Transforms UPLC Term → Scala 3 code (Expr) 
   - Uses `ContinuationJitRepr` (Return/Apply/Force)
   - Generates heap-based continuation objects
   - Compiles to JVM bytecode

2. **Runtime Execution**
   - Top-level iterative `eval` loop processes continuations
   - Pattern matching on continuation types
   - Array-based stack for frames
   - Function applications through `Any => Any`

### Performance Metrics (from ProfileAnalysisGC.md)

| Metric | CEK | JIT | Ratio |
|--------|-----|-----|-------|
| **Time/op** | 317.4 μs | 210.5 μs | 1.51x faster |
| **Alloc/op** | 1.34 MB | 547 KB | 2.45x less |
| **GC count** | 197 | 351 | 1.78x more |

**Analysis:** JIT is only 1.51x faster than CEK - not enough improvement for the complexity.

### Root Causes of Poor Performance

#### 1. **Excessive Indirection**
- All operations go through `ContinuationJitRepr.eval` loop
- Pattern matching overhead on every operation
- No direct execution paths for simple cases
- Type erasure requires `asInstanceOf` casts everywhere

#### 2. **Continuation Overhead**
- Every `Apply` creates continuation objects: `Apply(func, arg)`
- Every function returns `Return(value)` wrapper
- Frames allocated on heap: `ApplyFuncFrame`, `ApplyArgFrame`, `ForceFrame`
- Loop unpacks and repacks continuations repeatedly

#### 3. **Function Application Model**
```scala
// Current: Multiple indirections
Apply(func, arg) → 
  pushFrame(ApplyFuncFrame) → 
  eval(func) → Return(f) → 
  pushFrame(ApplyArgFrame) → 
  eval(arg) → Return(a) → 
  f.asInstanceOf[Any => Any](a)
```

#### 4. **Lack of Specialization**
- All values are `Any` type
- No specialized paths for primitives (BigInt, Boolean, ByteString)
- No inline of common builtin operations
- Budget tracking on every operation

#### 5. **Staging Limitations**
- Scala 3 staging has limited optimization capabilities
- Generated code is not as optimized as hand-written JVM bytecode
- No control over JIT compiler hints (@inline, @tailrec)
- Large generated methods may not get JIT compiled

## Proposed Re-engineering Strategies

### Strategy 1: Direct Bytecode Generation (Recommended)

**Approach:** Replace Scala 3 staging with direct JVM bytecode generation using ASM library.

**Benefits:**
- Full control over generated bytecode
- Can emit optimized instruction sequences
- Direct method calls without continuation wrappers
- Specialized methods for different value types
- Can use JVM invokedynamic for dynamic dispatch

**Implementation:**
```scala
// Example transformation
Term.Apply(Term.Builtin(AddInteger), Term.Const(1), Term.Const(2))

// Current: Generates continuation chain
Return((x: Any) => Return((y: Any) => x.asInstanceOf[BigInt] + y.asInstanceOf[BigInt]))

// Proposed: Direct bytecode
ALOAD 1  // load x
CHECKCAST java/math/BigInteger
ALOAD 2  // load y
CHECKCAST java/math/BigInteger
INVOKEVIRTUAL java/math/BigInteger.add
ARETURN
```

**Architecture:**
```
UPLC Term → Bytecode Generator → Class Definition → JVM ClassLoader
                ↓
     Use ASM library for bytecode emission
     Generate specialized methods
     Inline common operations
```

**Estimated Improvement:** 3-5x faster than current JIT (1000-1600 μs → 200-300 μs → 60-100 μs)

---

### Strategy 2: Three-Tier Compilation

**Approach:** Hybrid interpreter/JIT with hotspot detection.

**Tiers:**
1. **Tier 0: Fast Interpreter** - Optimized interpreter like CekMachine but faster
2. **Tier 1: Template JIT** - Quick compilation with templates
3. **Tier 2: Optimizing JIT** - Full optimization with bytecode generation

**Benefits:**
- Fast startup (no compilation cost initially)
- JIT only hot code paths
- Can use runtime profiling data for optimization
- Gradual optimization like HotSpot JVM

**Implementation:**
```scala
class TieredJIT {
  private val executionCounts = mutable.HashMap[Term, Int]()
  private val tier1Cache = mutable.HashMap[Term, CompiledCode]()
  private val tier2Cache = mutable.HashMap[Term, OptimizedCode]()
  
  def evaluate(term: Term): Any = {
    val count = executionCounts.getOrElseUpdate(term, 0) + 1
    executionCounts(term) = count
    
    if count < 10 then
      // Tier 0: Interpret
      interpretTerm(term)
    else if count < 100 then
      // Tier 1: Template JIT
      tier1Cache.getOrElseUpdate(term, compileTemplate(term))
        .execute()
    else
      // Tier 2: Optimizing JIT
      tier2Cache.getOrElseUpdate(term, compileOptimized(term))
        .execute()
  }
}
```

**Estimated Improvement:** 2-3x faster than current JIT on hot paths

---

### Strategy 3: Specialized Interpreter with Threaded Code

**Approach:** Replace continuation-based eval with direct-threaded interpreter.

**Key Ideas:**
- Pre-compile UPLC to internal bytecode format
- Use switch-based dispatch or computed goto pattern
- Inline hot operations
- Reduce object allocations

**Example Bytecode:**
```scala
enum OpCode {
  case CONST(value: Any)
  case VAR(index: Int)
  case APPLY
  case FORCE
  case BUILTIN_ADD_INTEGER
  case BUILTIN_MULTIPLY_INTEGER
  // ... specialized opcodes for each builtin
}

class CompiledProgram(val opcodes: Array[OpCode], val constants: Array[Any])

def evaluate(program: CompiledProgram): Any = {
  val stack = new Array[Any](256)
  var sp = 0  // stack pointer
  var ip = 0  // instruction pointer
  
  while ip < program.opcodes.length do {
    (program.opcodes(ip): @switch) match {
      case OpCode.CONST(v) =>
        stack(sp) = v
        sp += 1
        ip += 1
      case OpCode.BUILTIN_ADD_INTEGER =>
        val b = stack(sp - 1).asInstanceOf[BigInt]
        val a = stack(sp - 2).asInstanceOf[BigInt]
        sp -= 1
        stack(sp - 1) = a + b
        ip += 1
      // ... other opcodes
    }
  }
  stack(0)
}
```

**Benefits:**
- Much simpler than continuation-based approach
- Direct dispatch, minimal indirection
- Stack-based execution is cache-friendly
- Easy to optimize specific opcodes

**Estimated Improvement:** 2-4x faster than current JIT

---

### Strategy 4: LLVM Backend

**Approach:** Compile UPLC to LLVM IR, let LLVM optimize.

**Benefits:**
- State-of-the-art optimizations
- Native code generation
- Cross-platform support
- Extensive optimization passes

**Challenges:**
- Complex integration
- Requires LLVM installation
- May have long compilation times
- Interop with Scala/JVM may be tricky

**Estimated Improvement:** 5-10x faster (if done well)

---

## Recommended Approach: Hybrid Strategy

Combine elements from multiple strategies for best results:

### Phase 1: Quick Wins (1-2 weeks)
**Goal:** 2x improvement with minimal changes

1. **Optimize Current Continuation-Based Approach**
   - Pre-allocate frame arrays instead of dynamic growth
   - Add fast-path for `Return` with empty stack
   - Inline common builtin operations directly in generated code
   - Reduce pattern matching overhead with type tests

```scala
// Instead of:
case Return(value) =>
  if stackSize == 0 then return value

// Use:
if current.isInstanceOf[Return] && stackSize == 0 then
  return current.asInstanceOf[Return].value
```

2. **Specialize Hot Operations**
   - Generate specialized methods for integer arithmetic
   - Direct paths for data comparisons
   - Inline IfThenElse

3. **Reduce Allocations**
   - Pool continuation objects
   - Reuse frame arrays
   - Use value classes where possible

### Phase 2: Bytecode Generation (3-4 weeks)
**Goal:** 3-5x improvement total

1. **Implement ASM-based Bytecode Generator**
   - Start with simple cases (Const, Var, Apply)
   - Add builtin operations
   - Implement control flow (Delay, Force, Case)

2. **Generate Specialized Methods**
   - Type-specific method signatures
   - Primitive specialization (@specialized)
   - Direct method calls

3. **Optimization Passes**
   - Constant folding
   - Dead code elimination  
   - Inline expansion

### Phase 3: Threaded Interpreter Fallback (2-3 weeks)
**Goal:** Fast startup for simple/one-time scripts

1. **Implement Bytecode Format**
   - Internal OpCode representation
   - Compilation from UPLC
   - Stack-based evaluation

2. **Hybrid Execution**
   - Interpret first few executions
   - Collect profiling data
   - JIT compile hot functions

## Implementation Plan

### Week 1-2: Profiling and Quick Optimizations
- [ ] Deep profiling of current JIT (JFR, async-profiler)
- [ ] Identify hotspots in eval loop
- [ ] Implement frame array pre-allocation
- [ ] Add fast-path for Return handling
- [ ] Inline 5-10 most common builtins

**Target: 2x improvement (210μs → 105μs)**

### Week 3-4: ASM Foundation
- [ ] Add ASM dependency
- [ ] Implement basic bytecode generator for Const/Var
- [ ] Generate simple methods
- [ ] Test and validate correctness

### Week 5-6: Complete Bytecode Generator
- [ ] Implement all Term cases
- [ ] Add all builtin operations
- [ ] Handle Constr/Case
- [ ] Integration testing

**Target: 3x improvement (210μs → 70μs)**

### Week 7-8: Optimization
- [ ] Specialization for primitive types
- [ ] Inline common patterns
- [ ] Optimize budget tracking
- [ ] Performance testing and tuning

**Target: 5x improvement (210μs → 40μs)**

## Success Metrics

| Metric | Current | Target Phase 1 | Target Phase 2 | Target Phase 3 |
|--------|---------|----------------|----------------|----------------|
| **Time/op** | 210 μs | 105 μs (2x) | 70 μs (3x) | 40 μs (5x) |
| **vs CEK** | 1.51x | 3x | 4.5x | 8x |
| **Alloc/op** | 547 KB | 300 KB | 150 KB | 50 KB |

## Risks and Mitigations

### Risk 1: Bytecode Generation Complexity
**Mitigation:** Start with simple cases, incremental implementation, extensive testing

### Risk 2: Correctness Issues
**Mitigation:** Comprehensive test suite, differential testing against CEK, property-based tests

### Risk 3: Maintenance Burden
**Mitigation:** Well-documented code, clear separation of concerns, automated tests

### Risk 4: Platform Dependencies
**Mitigation:** Keep CekMachine as fallback, platform-specific implementations

## Alternative: Stop Using JIT

If re-engineering proves too costly, consider:
1. **Optimize CekMachine Instead**
   - Focus on making interpreter faster
   - Simpler, less risky
   - May achieve similar results

2. **Use Native Code Generator**
   - Compile UPLC to Scala Native
   - Ahead-of-time compilation
   - Better performance, simpler implementation

## Conclusion

The current JIT compiler is not delivering sufficient performance gains. A major re-engineering focusing on **direct bytecode generation** is recommended to achieve 3-5x performance improvement. The phased approach allows for incremental progress with measurable results at each stage.

**Recommended Action:** Proceed with Phase 1 (quick optimizations) immediately to validate approach, then commit to Phase 2 (bytecode generation) based on results.

## References

- Current Implementation: `scalus-uplc-jit-compiler/src/main/scala/scalus/uplc/eval/JIT.scala`
- CEK Machine: `scalus-core/shared/src/main/scala/scalus/uplc/eval/Cek.scala`
- Performance Analysis: `scalus-uplc-jit-compiler/docs/ProfileAnalysisGC.md`
- ASM Library: https://asm.ow2.io/
- JVM Bytecode Spec: https://docs.oracle.com/javase/specs/jvms/se17/html/
